# Build: Personalized Datadog Copilot (Modern UI) - One-shot implementation prompt for Cursor

## Goal
Build a hackathon-ready, production-style web app that makes Datadog feel personalized per engineer.
The product learns how a user investigates incidents and then:
- Generates a personalized “System View” home dashboard
- Provides an “Incident Room” with guided next-step pivots (one-click investigation shortcuts)
- Produces “Recommendations” to tune monitors/dashboards and attach verification tests
- Learns continuously from user accept/reject actions and stores preferences in memory

Hard requirements:
- Backend and frontend fully separate (two folders, separate package managers)
- Multi-agent backend using **Minimax** for all agent reasoning and orchestration
- Heavy usage of **Datadog MCP** for reading telemetry and powering flows
- Use **TestSprite** to generate and run verification checks for recommended changes
- Modern, engaging UI with CopilotKit-like interactive panels (you will implement the UI, do not depend on CopilotKit library unless trivial)

Deliver a working demo with seeded mock mode if Datadog credentials are not present.

## Non-goals
- Not building an auto-remediation agent that changes production infra
- Not implementing full Datadog asset writes if APIs are complicated; focus on read + propose + export payloads that could be applied

---

## Tech Stack
### Frontend
- Next.js (App Router) + TypeScript
- TailwindCSS + shadcn/ui
- Framer Motion (micro-animations)
- Recharts (charts)
- React Query (TanStack Query) for data fetching
- Monaco editor for showing generated queries and “export payloads”
- Authentication: simple demo login (email + role), stored in localStorage (hackathon), plus server session token

### Backend
- Python FastAPI
- Minimax LLM for agent reasoning (all agent text generation, classification, summaries, recommendations)
- Datadog MCP integration (read-only actions)
- Memory store: Postgres (preferred) or SQLite in demo mode
- Redis optional for caching, but not required
- Background jobs: simple async tasks using FastAPI background tasks (or Celery optional)

### Integrations
- Datadog MCP (required)
- TestSprite (required)
- Slack (optional but recommended for demo notifications)
- GitHub (optional, only for deploy markers; can stub)

---

## Repo layout
Create a mono-repo structure:

/frontend
  package.json
  next.config.js
  src/app/...
  src/components/...
  src/lib/api.ts
  src/lib/auth.ts
  src/styles/...

/backend
  pyproject.toml (or requirements.txt)
  app/main.py
  app/core/config.py
  app/core/logging.py
  app/db/session.py
  app/db/models.py
  app/db/migrations/ (optional)
  app/agents/ (all minimax agents here)
  app/integrations/datadog_mcp.py
  app/integrations/testsprite.py
  app/routes/...
  app/services/...
  app/schemas/...

/README.md
/docker-compose.yml (optional but recommended)

---

## Product UX: Screens and flows (must implement)

### 1) Login
- Simple email + role select (SRE, Backend, ML, Product)
- Store a userId returned by backend
- Route to Home

### 2) Home: “Your System View”
A modern, card-based dashboard that feels personalized:
- Header: greeting + current focus badge (e.g., “Latency Watch”, “Quality Watch”)
- Left column:
  - “Services You Touch” (chips)
  - “Your Top Endpoints” (list with sparkline)
  - “Your Investigation Shortcuts” (buttons)
- Main area:
  - “Live Health” charts: error rate, p95 latency, throughput for top endpoints
  - “Active Alerts” feed (from Datadog monitors)
  - “Recent Incidents” timeline
- Right column:
  - “Learned Patterns” (natural language bullets)
  - “Suggested Improvements” (monitor tuning, dashboard correlations, new SLOs)
  - “Confidence” meter per suggestion

Key UX detail:
- Every suggestion must be actionable with buttons: Preview, Accept, Reject.
- Preview opens a side panel showing evidence and the export payload (JSON) plus tests.

### 3) Incident Room
When a user clicks an active alert or recent incident:
- Big incident header: severity, start time, affected services, blast radius estimate
- Tabs: Overview, Evidence, Guided Steps, Recommendations, Tests, Memory

Guided Steps is the signature:
- Show 3-5 “Next best step” buttons generated by the backend personalization.
Examples:
  - “Filter traces by endpoint and show top slow spans”
  - “Compare retriever latency vs thumbs-down rate”
  - “Show deploy markers for these services last 60 minutes”
- Clicking a step runs a backend action and renders results immediately.

Evidence tab:
- Traces summary
- Log snippets
- Metric correlations
- Provide a mini “investigation graph” showing the steps taken (nodes) with timestamps.

Recommendations tab:
- Ranked hypotheses (with confidence)
- Proposed changes (monitor tuning, dashboards, SLOs, query templates)
- Each has “Export” payload and “Generate Tests”

Tests tab:
- Run TestSprite tests
- Show pass/fail, artifacts, and a “save to regression suite” button

Memory tab:
- Shows what the system learned from this incident and the user’s decisions

### 4) Personalization Studio
A dedicated screen to manage personalization:
- “My Preferences”
  - action style: conservative vs aggressive
  - noise tolerance: low vs high
  - focus areas: infra vs LLM quality vs both
- “My Shortcuts”
  - reorder, pin, rename shortcuts
- “My Patterns”
  - edit learned patterns with thumbs up/down
- “Export”
  - export memory profile as JSON

Make this UI playful, modern, and interactive:
- drag and drop for shortcut reordering
- animated confidence bars
- clean typography and spacing

---

## Backend: Core concept and data model
Backend is a read-and-recommend system.

### Entities
- User
  - id, email, role, created_at
- OrgConfig
  - datadog_site, default_time_window, tags_of_interest
- InvestigationSession
  - id, user_id, started_at, ended_at, context (json)
- InvestigationEvent
  - id, session_id, kind (open_dashboard, run_query, click_trace, filter_logs, etc)
  - payload (json), created_at
- Incident
  - id, source (datadog), title, started_at, severity, services (json), state
- Recommendation
  - id, user_id, incident_id nullable, type (monitor_tune, dashboard, slo, shortcut, hypothesis)
  - content (json), confidence, status (proposed, accepted, rejected)
- MemoryProfile
  - user_id
  - preferences (json)
  - patterns (json)
  - shortcuts (json)
  - success_map (json)  // incident signature -> preferred steps

Use Postgres if possible. SQLite acceptable in demo mode.

---

## Multi-agent design (Minimax required for reasoning)
All agent calls use Minimax.

Implement these agents (each in its own file under /backend/app/agents):

1) BehaviorMinerAgent
- Input: user events, recent sessions, accepted/rejected recs
- Output: updated patterns and preference adjustments

2) IncidentSummarizerAgent
- Input: telemetry bundle (metrics, traces, logs, monitors)
- Output: incident envelope (what happened, when, where, blast radius)

3) GuidedStepsAgent
- Input: incident envelope + memory profile + telemetry summary
- Output: 3-7 guided steps with rationale and required backend action type

4) HypothesisRankerAgent
- Input: telemetry evidence + known patterns
- Output: ranked hypotheses with confidence and cited evidence pointers

5) RecommendationDesignerAgent
- Input: hypotheses + user preferences
- Output: proposals (monitor tuning suggestions, dashboards correlation panels, SLO suggestions, query templates, shortcut templates) each with export payload

6) TestPlanAgent
- Input: recommendation + incident type + telemetry signature
- Output: TestSprite test plan, including what to validate and how to interpret failures

Important:
- Every agent output must be strict JSON with a schema. Enforce with pydantic validation.
- The system must be deterministic enough for demo: use temperature low and consistent prompts.

---

## Datadog MCP integration (heavy usage)
Create /backend/app/integrations/datadog_mcp.py exposing functions:

Required read operations:
- get_active_monitors(time_window, tags)
- get_monitor_details(monitor_id)
- query_metrics(query, from_ts, to_ts)
- search_logs(query, from_ts, to_ts, limit)
- fetch_traces(service, resource, from_ts, to_ts, limit)
- get_service_dependencies(service, from_ts, to_ts)
- get_deploy_markers(from_ts, to_ts) (optional)

If MCP is not available, fallback to mock provider:
- Provide deterministic mock telemetry bundles under /backend/app/integrations/mock_data/
- Toggle with env var DD_MODE=mock

---

## TestSprite integration
Create /backend/app/integrations/testsprite.py:
- create_test_plan(name, steps_json)
- run_test_plan(plan_id)
- get_test_results(run_id)

If TestSprite API not available, provide mock runs:
- deterministic pass/fail based on incident signature hash

---

## API design (FastAPI)
All endpoints JSON. Use /api prefix.

Auth
- POST /api/auth/login {email, role} -> {token, user}
- GET /api/me -> {user, memoryProfile}

Home
- GET /api/home/overview -> personalized overview bundle
  - servicesYouTouch, topEndpoints, liveChartsData, activeAlerts, recentIncidents, learnedPatterns, suggestedImprovements

Incidents
- GET /api/incidents -> list
- GET /api/incidents/{id} -> incident envelope + initial evidence + guided steps + recs
- POST /api/incidents/from-monitor {monitor_id} -> create or attach incident

Guided steps execution
- POST /api/incidents/{id}/steps/execute
  - body: {step_id}
  - returns: {result, updated_investigation_graph, next_steps_optional}

Recommendations
- GET /api/recommendations?incident_id=&status=
- POST /api/recommendations/{id}/accept
- POST /api/recommendations/{id}/reject
- GET /api/recommendations/{id}/export -> payload JSON + copyable CLI snippets

Tests
- POST /api/tests/generate {recommendation_id} -> {plan}
- POST /api/tests/run {plan_id} -> {run_id}
- GET /api/tests/runs/{run_id} -> results

Memory
- GET /api/memory/profile
- POST /api/memory/preferences
- POST /api/memory/shortcuts/reorder
- POST /api/memory/patterns/feedback

Observability
- POST /api/telemetry/bundle
  - returns raw telemetry used for transparency

---

## Frontend UI requirements (make it modern and engaging)
Global:
- Dark mode first with tasteful gradients
- Glassmorphism panels for side drawers
- Micro-animations: hover lift, loading shimmer, smooth transitions
- A command palette (Cmd+K) to jump to incidents, services, shortcuts
- A right-side “Copilot Panel” persistent drawer that shows:
  - what it thinks is happening
  - the next suggested action
  - the evidence links
  - accept/reject controls

Components to build:
- AppShell with sidebar navigation
- AlertFeed with severity pills
- IncidentTimeline
- GuidedStepButtons (animated)
- EvidenceCards (logs, traces, metrics)
- ConfidenceMeter
- RecommendationCard with Preview drawer
- Monaco payload viewer for export
- InvestigationGraph visualization (simple nodes and edges, React Flow optional)

Pages:
- /login
- /home
- /incidents
- /incidents/[id]
- /studio (personalization)

---

## Personalization logic (must be real, not fake)
Implement a simple but real learning loop:
- Start with default shortcuts and preferences based on role
- When user accepts a recommendation:
  - increase weight for similar future suggestions
- When user rejects:
  - decrease weight and record reason (optional)
- When user executes guided steps:
  - record sequence and boost those steps for similar incidents
- Define incident signature:
  - (service, endpoint, primary symptom type, top error code, deploy marker presence)
- Use signature to retrieve best next steps and preferred rec types

This should produce visibly different behavior per user after a few interactions.

---

## Security and safety
- Read-only integrations by default
- Never execute destructive actions
- Tokens stored in httpOnly cookies if possible, else bearer token for demo
- Redact secrets in logs
- Rate limit API endpoints lightly

---

## Environment variables
Frontend (/frontend/.env.local)
- NEXT_PUBLIC_API_BASE=http://localhost:8000/api

Backend (/backend/.env)
- APP_ENV=dev
- DD_MODE=mock|live
- DATADOG_SITE=
- DATADOG_API_KEY= (if needed by MCP, else omit)
- DATADOG_APP_KEY= (if needed)
- MINIMAX_API_KEY=
- MINIMAX_MODEL= (set a sensible default)
- TESTSPRITE_API_KEY=
- DATABASE_URL=postgresql+psycopg://... or sqlite:///./app.db
- SLACK_WEBHOOK_URL= (optional)

---

## Demo mode
Must run with no credentials:
- DD_MODE=mock
- TestSprite mock
- Still show full UX and agent behaviors using mock telemetry bundles

Include a “Simulate Incident” button on Home:
- Triggers a new incident created from mock monitor
- Navigates to Incident Room
- Lets judges experience the guided flow

---

## Implementation steps inside Cursor (do all in one shot)
1) Create repo structure with frontend and backend
2) Backend:
   - FastAPI app, CORS, auth
   - DB models + CRUD
   - Datadog MCP provider + mock provider
   - TestSprite provider + mock provider
   - Minimax agent wrappers + pydantic JSON schemas
   - Routes implementing the endpoints
   - Seed data for mock mode
3) Frontend:
   - Next.js app router pages
   - App shell, theme, components
   - Data fetching via React Query
   - Modern UI, animations, drawers, command palette
   - Incident Room UX with guided step executions
   - Personalization Studio with drag and drop
4) Provide scripts:
   - backend: uvicorn run
   - frontend: next dev
5) Provide README with setup and demo walkthrough

---

## Output quality bar
- Code should be clean, typed, modular
- No placeholder UI, must look like a modern SaaS
- Every agent output validated against schemas
- Provide at least 2 mock incident scenarios:
  1) Infra latency spike (DB lock style)
  2) AI quality drift (retrieval mismatch style)
- Ensure personalization changes are visible after a few clicks

---

## Final note
Use Minimax for all reasoning and text generation.
Datadog MCP must be used heavily in live mode and simulated realistically in mock mode.
Keep backend and frontend completely separate and runnable independently.